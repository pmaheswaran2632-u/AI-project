# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a2w6l0Als-gC1znqKR8DGkxFIQLKxfdg
"""

"""
advanced_time_series_forecasting.py

Advanced Time Series Forecasting with Neural Networks and Explainability
- Generates a synthetic multivariate, non-stationary seasonal dataset (>=1000 rows)
- Builds and hyperparameter-tunes an LSTM model for multi-step forecasting
- Trains final model, evaluates vs. an ARIMA baseline (univariate on target)
- Computes RMSE/MAE for predictions and demonstrates SHAP explanations
- Contains docstrings and type hints for production use
"""

import os
import random
from typing import Tuple, Dict, Any

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Keras Tuner (pip install keras-tuner)
import keras_tuner as kt

# statsmodels for baseline ARIMA
import statsmodels.api as sm

# SHAP for explainability (pip install shap)
import shap

SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)


def generate_multivariate_timeseries(n_steps: int = 1500, n_features: int = 4) -> pd.DataFrame:
    """
    Generate a synthetic complex multivariate time series with non-stationarity and seasonal components.
    The target column will be named "y", other features "x1", "x2", ...
    Args:
        n_steps: Number of time steps (rows). Should be >= 1000 per requirement.
        n_features: Total number of features including target (so target + n_features-1 exog).
    Returns:
        pd.DataFrame: time-indexed DataFrame with columns ['y','x1',...'x{n_features-1}'].
    """
    t = np.arange(n_steps)
    # Trend (non-linear)
    trend = 0.0005 * (t ** 1.8)

    # Seasonal components (different frequencies)
    seasonal1 = 2.0 * np.sin(2 * np.pi * t / 24)         # daily-ish seasonality
    seasonal2 = 1.5 * np.sin(2 * np.pi * t / 168)        # weekly-ish seasonality
    seasonal3 = 0.8 * np.sin(2 * np.pi * t / 12 + 0.2)   # another frequency

    # Heteroscedastic noise (increasing variance)
    noise = (0.3 + 0.001 * t) * np.random.randn(n_steps)

    # Additional exogenous features (some are correlated)
    x1 = seasonal1 + 0.1 * trend + 0.5 * np.random.randn(n_steps)                # correlated seasonal
    x2 = 0.5 * seasonal2 + 0.3 * trend + np.random.randn(n_steps)               # correlated different freq
    x3 = 0.2 * seasonal3 + 0.2 * np.sin(2 * np.pi * t / 50) + 0.6 * np.random.randn(n_steps)
    extras = []
    for i in range(n_features - 4):
        extras.append(0.5 * np.sin(2 * np.pi * t / (20 + 5 * i)) + 0.7 * np.random.randn(n_steps))

    # Target y as combination of signals + some regime change
    regime = np.where(t > n_steps * 0.6, 1.2, 1.0)  # small regime shift after 60% of series
    y = regime * (1.8 * seasonal1 + 0.6 * seasonal2 + 0.4 * trend) + 0.8 * noise

    data = {'y': y, 'x1': x1, 'x2': x2, 'x3': x3}
    for idx, ex in enumerate(extras):
        data[f'x{4 + idx}'] = ex

    df = pd.DataFrame(data)
    df.index = pd.RangeIndex(start=0, stop=n_steps, step=1)
    return df


def create_sequences(df: pd.DataFrame, input_steps: int, forecast_horizon: int,
                     target_col: str = 'y') -> Tuple[np.ndarray, np.ndarray]:
    """
    Create sliding-window sequences for supervised learning.
    Args:
        df: DataFrame of features (already scaled).
        input_steps: Number of past steps used as input.
        forecast_horizon: Number of steps to forecast (multi-step).
        target_col: the column name to predict (the model outputs forecast_horizon series).
    Returns:
        X: (n_samples, input_steps, n_features)
        y: (n_samples, forecast_horizon)
    """
    data = df.values
    n_samples = len(df) - input_steps - forecast_horizon + 1
    X = np.zeros((n_samples, input_steps, df.shape[1]))
    y = np.zeros((n_samples, forecast_horizon))
    target_idx = list(df.columns).index(target_col)

    for i in range(n_samples):
        X[i] = data[i:i + input_steps]
        y[i] = data[i + input_steps: i + input_steps + forecast_horizon, target_idx]

    return X, y


def build_lstm_model(hp: kt.HyperParameters, input_shape: Tuple[int, int], forecast_horizon: int) -> keras.Model:
    """
    Build an LSTM model using hyperparameters from Keras Tuner.
    Args:
        hp: HyperParameters object (used by tuner).
        input_shape: (timesteps, n_features)
        forecast_horizon: output steps
    Returns:
        Compiled Keras model.
    """
    model = keras.Sequential()
    # LSTM layer count
    n_layers = hp.Int('lstm_layers', 1, 3, default=1)
    for i in range(n_layers):
        units = hp.Int(f'units_{i}', min_value=16, max_value=256, step=16, default=64)
        return_sequences = (i < n_layers - 1)
        if i == 0:
            model.add(layers.LSTM(units, return_sequences=return_sequences, input_shape=input_shape))
        else:
            model.add(layers.LSTM(units, return_sequences=return_sequences))
        if hp.Boolean(f'dropout_layer_{i}', default=True):
            rate = hp.Float(f'dropout_rate_{i}', 0.0, 0.5, step=0.05, default=0.2)
            model.add(layers.Dropout(rate))

    model.add(layers.Dense(units=hp.Int('dense_units', 8, 128, step=8, default=32), activation='relu'))
    model.add(layers.Dense(forecast_horizon))  # linear output for regression

    lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=['mae'])
    return model


def run_hyperparameter_search(X_train: np.ndarray, y_train: np.ndarray,
                              X_val: np.ndarray, y_val: np.ndarray,
                              input_shape: Tuple[int, int], forecast_horizon: int,
                              max_trials: int = 10, executions_per_trial: int = 1) -> Dict[str, Any]:
    """
    Run Keras Tuner RandomSearch to find hyperparameters for the LSTM model.
    Args:
        X_train, y_train, X_val, y_val: training and validation sets
        input_shape: (timesteps, n_features)
        forecast_horizon: number of output steps
        max_trials: number of hyperparameter trials
    Returns:
        dict with best hyperparameters and tuner object.
    """
    tuner = kt.RandomSearch(
        lambda hp: build_lstm_model(hp, input_shape, forecast_horizon),
        objective=kt.Objective('val_loss', direction='min'),
        max_trials=max_trials,
        executions_per_trial=executions_per_trial,
        project_name='ts_lstm_tuning',
        overwrite=True,
        seed=SEED
    )
    stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)
    tuner.search(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val),
                 callbacks=[stop_early], verbose=1)
    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]
    return {'tuner': tuner, 'best_hp': best_hp}


def train_final_model(best_hp: kt.HyperParameters, input_shape: Tuple[int, int], forecast_horizon: int,
                      X_train: np.ndarray, y_train: np.ndarray,
                      X_val: np.ndarray, y_val: np.ndarray) -> keras.Model:
    """
    Train final LSTM model using best hyperparameters.
    Returns the trained model.
    """
    model = build_lstm_model(best_hp, input_shape, forecast_horizon)
    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]
    history = model.fit(
        X_train, y_train, epochs=200, batch_size=32,
        validation_data=(X_val, y_val), callbacks=callbacks, verbose=1
    )
    return model


def evaluate_forecasts(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
    """
    Evaluate multi-step forecasts with RMSE and MAE aggregated across horizon.
    y_true, y_pred: shape (n_samples, horizon)
    Returns aggregated RMSE and MAE.
    """
    rmse = np.sqrt(mean_squared_error(y_true.flatten(), y_pred.flatten()))
    mae = mean_absolute_error(y_true.flatten(), y_pred.flatten())
    return {'RMSE': float(rmse), 'MAE': float(mae)}


def baseline_arima_forecast(series: pd.Series, train_end: int, forecast_horizon: int) -> np.ndarray:
    """
    Fit a simple SARIMAX (ARIMA) on the training portion of the univariate target and produce rolling forecasts.
    Args:
        series: full pd.Series of the target (not scaled).
        train_end: index at which training ends (exclusive or inclusive depending usage).
        forecast_horizon: steps to forecast ahead per evaluation.
    Returns:
        numpy array of shape (n_test_samples, forecast_horizon) aligned for comparison with model outputs.
    """
    # We'll do a simple strategy: for each test window starting at idx i, fit on data[:i] and forecast horizon.
    n = len(series)
    input_steps = 0  # we will expand training progressively
    start_test = train_end
    forecasts = []
    for i in range(start_test, n - forecast_horizon + 1):
        train_series = series[:i]  # fit on all data up to i-1
        try:
            model = sm.tsa.statespace.SARIMAX(train_series, order=(1, 1, 1), enforce_stationarity=False, enforce_invertibility=False)
            res = model.fit(disp=False)
            fc = res.forecast(steps=forecast_horizon)
        except Exception:
            # fallback: naive last value repeated
            fc = np.repeat(train_series.iloc[-1], forecast_horizon)
        forecasts.append(fc.values)
    return np.array(forecasts)


def explain_with_shap(model: keras.Model, X_background: np.ndarray, X_explain: np.ndarray, feature_names: list):
    """
    Use SHAP KernelExplainer to explain the model predictions for a set of examples.
    KernelExplainer is model-agnostic but slow; for small samples it's acceptable.
    Args:
        model: compiled keras model
        X_background: background dataset for SHAP (e.g., subset of train)
        X_explain: examples to explain (shape (m, timesteps, n_features))
        feature_names: list of column names matching last axis
    Returns:
        shap_values: list/array of SHAP values for each explained example
    """
    # Wrap the model prediction to accept 2D flattened arrays for KernelExplainer
    def model_predict_flat(x_flat):
        # x_flat shape: (n_samples, timesteps * n_features)
        n_samples = x_flat.shape[0]
        x_reshaped = x_flat.reshape(n_samples, X_background.shape[1], X_background.shape[2])
        preds = model.predict(x_reshaped)
        # SHAP expects (n_samples, outputs). We'll sum across horizon for simplicity or explain the first horizon.
        # We'll explain the sum of horizon predictions to get feature importance across horizon
        return preds.sum(axis=1)

    # Prepare flat background and explanation arrays
    Xb_flat = X_background.reshape(X_background.shape[0], -1)
    Xe_flat = X_explain.reshape(X_explain.shape[0], -1)

    explainer = shap.KernelExplainer(model_predict_flat, Xb_flat[:100])  # use up to 100 background rows
    shap_values = explainer.shap_values(Xe_flat, nsamples=200)
    # shap_values shape: for regression KernelExplainer returns list (single output) or array
    return shap_values


def main():
    # 1) Generate data
    df = generate_multivariate_timeseries(n_steps=1500, n_features=6)  # 1500 >= 1000
    print("Generated data shape:", df.shape)
    print(df.head())

    # 2) Train/Val/Test split indexes
    n = len(df)
    train_frac, val_frac = 0.7, 0.15
    train_end = int(n * train_frac)
    val_end = int(n * (train_frac + val_frac))

    # 3) Scaling (fit on train only)
    scaler = StandardScaler()
    scaler.fit(df.iloc[:train_end])
    df_scaled = pd.DataFrame(scaler.transform(df), columns=df.columns, index=df.index)

    # 4) Create sequences
    input_steps = 48  # use 48 past steps (e.g., 2 days if hourly data)
    forecast_horizon = 10  # multi-step ahead
    X, y = create_sequences(df_scaled, input_steps=input_steps, forecast_horizon=forecast_horizon, target_col='y')

    # Align train/val/test split on sequence level
    # first sequence starts at index 0 -> corresponds to original series index range [0, input_steps+forecast_horizon-1]
    seq_train_end = train_end - input_steps - forecast_horizon + 1
    seq_val_end = val_end - input_steps - forecast_horizon + 1

    X_train, y_train = X[:seq_train_end], y[:seq_train_end]
    X_val, y_val = X[seq_train_end:seq_val_end], y[seq_train_end:seq_val_end]
    X_test, y_test = X[seq_val_end:], y[seq_val_end:]

    print("Sequence shapes:", X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)

    # 5) Hyperparameter search
    input_shape = (X_train.shape[1], X_train.shape[2])
    tuner_results = run_hyperparameter_search(X_train, y_train, X_val, y_val, input_shape, forecast_horizon,
                                              max_trials=8, executions_per_trial=1)
    best_hp = tuner_results['best_hp']
    print("Best hyperparameters found:")
    for key in best_hp.values.keys():
        print(key, "=", best_hp.get(key))

    # 6) Train final model
    model = train_final_model(best_hp, input_shape, forecast_horizon, X_train, y_train, X_val, y_val)
    model.summary()

    # 7) Evaluate on test set
    y_pred_test = model.predict(X_test)
    perf = evaluate_forecasts(y_test, y_pred_test)
    print("LSTM test performance:", perf)

    # 8) Baseline ARIMA (univariate on original unscaled target)
    # For alignment, we build forecasts for the same test starting points used above.
    # Compute baseline forecasts on original series (not scaled)
    series = df['y']
    # Map seq_val_end to original index to get starting test index for ARIMA forecasting
    # The first sequence used in X_test corresponds to original index: seq_idx = seq_val_end
    start_test_index = seq_val_end + input_steps  # the point where forecast begins for first test sample
    # We'll produce rolling ARIMA forecasts for the same number of test samples
    arima_forecasts = baseline_arima_forecast(series, start_test_index, forecast_horizon)
    # y_test corresponds to scaled targets; we need to inverse transform target component to original scale
    # Recover original y_test values (unscale)
    # For inverse, we need scaler mean/scale for 'y' column only
    y_mean = scaler.mean_[list(df.columns).index('y')]
    y_scale = scaler.scale_[list(df.columns).index('y')]
    y_test_unscaled = y_test * y_scale + y_mean
    # arima_forecasts shape should match y_test_unscaled shape; if length mismatch, align
    min_len = min(y_test_unscaled.shape[0], arima_forecasts.shape[0])
    arima_perf = evaluate_forecasts(y_test_unscaled[:min_len], arima_forecasts[:min_len])
    print("ARIMA baseline performance (on unscaled y):", arima_perf)

    # 9) Plot first several forecasts vs truth (for first test sample)
    samples_to_plot = 6
    fig, axs = plt.subplots(samples_to_plot, 1, figsize=(10, 3 * samples_to_plot))
    for i in range(samples_to_plot):
        if i >= len(y_test):
            break
        true = y_test_unscaled[i]
        pred_scaled = y_pred_test[i]
        pred_unscaled = pred_scaled * y_scale + y_mean
        ax = axs[i] if samples_to_plot > 1 else axs
        ax.plot(range(forecast_horizon), true, marker='o', label='True')
        ax.plot(range(forecast_horizon), pred_unscaled, marker='x', label='LSTM')
        if i < arima_forecasts.shape[0]:
            ax.plot(range(forecast_horizon), arima_forecasts[i], marker='d', label='ARIMA')
        ax.set_title(f'Test sample #{i} forecast (horizon={forecast_horizon})')
        ax.legend()
    plt.tight_layout()
    plt.show()

    # 10) SHAP explainability for a few examples (will be slower)
    # Use subset of training for background
    X_background = X_train[np.random.choice(X_train.shape[0], size=min(200, X_train.shape[0]), replace=False)]
    X_explain = X_test[:5]  # explain first 5 test windows
    feature_names = df.columns.tolist()
    print("Running SHAP KernelExplainer (this can be slow). Explaining the sum of horizon outputs per sample.")
    shap_values = explain_with_shap(model, X_background, X_explain, feature_names)

    # shap_values is for flattened input; reshape importance back to (timesteps, features) by summing absolute across timesteps
    shap_array = np.array(shap_values)
    # shap_array shape may be (n_samples, timesteps * n_features)
    if shap_array.ndim == 2:
        for i in range(shap_array.shape[0]):
            sv = shap_array[i]
            sv_matrix = sv.reshape(X_explain.shape[1], X_explain.shape[2])
            sv_abs = np.abs(sv_matrix).sum(axis=0)  # importance per feature
            print(f"Sample {i} feature importances (summed across timesteps):")
            for fname, imp in zip(feature_names, sv_abs):
                print(f"  {fname}: {imp:.4f}")
    else:
        print("Unexpected SHAP output shape:", shap_array.shape)

    # Save model and scaler for later production use
    model.save('lstm_multistep_model.h5')
    pd.to_pickle(scaler, 'scaler.pkl')
    df.to_csv('generated_multivariate_timeseries.csv', index=False)
    print("Saved model (lstm_multistep_model.h5), scaler (scaler.pkl), and dataset CSV.")